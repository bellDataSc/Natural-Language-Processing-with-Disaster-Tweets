{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Natural Language Processing with Disaster Tweets\n**Predict which Tweets are about real disasters and which ones are not**\n\nüìù Contribui√ß√£o\n\nEste reposit√≥rio ser√° continuamente atualizado com novas consultas e m√©todos de an√°lise. Sugest√µes e colabora√ß√µes s√£o bem-vindas!\n\nüìß Contato\n\nPara d√∫vidas ou colabora√ß√µes, entre em contato pelo LinkedIn ou via e-mail.\n\nüìå Autor: Bel - Analista de Dados T√©cnica PL - Governo do Estado de S√£o Paulo","metadata":{}},{"cell_type":"markdown","source":"# Passo 1: Importar Bibliotecas Necess√°rias","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T22:31:17.277175Z","iopub.execute_input":"2025-05-28T22:31:17.277633Z","iopub.status.idle":"2025-05-28T22:31:17.283765Z","shell.execute_reply.started":"2025-05-28T22:31:17.277592Z","shell.execute_reply":"2025-05-28T22:31:17.282509Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Passo 2: Carregar os Dados","metadata":{}},{"cell_type":"code","source":"# Carregar os dados de treino e teste\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Visualizar as primeiras linhas dos dados de treino\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T22:32:26.576562Z","iopub.execute_input":"2025-05-28T22:32:26.576941Z","iopub.status.idle":"2025-05-28T22:32:26.630553Z","shell.execute_reply.started":"2025-05-28T22:32:26.576910Z","shell.execute_reply":"2025-05-28T22:32:26.629491Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Passo 3: An√°lise Explorat√≥ria de Dados (EDA)","metadata":{}},{"cell_type":"code","source":"# Verificar a distribui√ß√£o das classes\nsns.countplot(train_data['target'])\nplt.title('Distribui√ß√£o das Classes')\nplt.show()\n\n# Verificar valores faltantes\nprint(train_data.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Passo 4: Pr√©-processamento de Texto","metadata":{}},{"cell_type":"code","source":"# Preencher valores faltantes\ntrain_data['text'] = train_data['text'].fillna('')\ntest_data['text'] = test_data['text'].fillna('')\n\n# Transformar o texto em vetores num√©ricos usando TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000)\nX_train = vectorizer.fit_transform(train_data['text'])\nX_test = vectorizer.transform(test_data['text'])\n\n# Separar os r√≥tulos\ny_train = train_data['target']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T19:52:48.208481Z","iopub.execute_input":"2025-03-11T19:52:48.208790Z","iopub.status.idle":"2025-03-11T19:52:48.488960Z","shell.execute_reply.started":"2025-03-11T19:52:48.208767Z","shell.execute_reply":"2025-03-11T19:52:48.487976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Passo 5: Treinar um Modelo B√°sico","metadata":{}},{"cell_type":"code","source":"# Dividir os dados de treino em treino e valida√ß√£o\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Treinar o modelo\nmodel = LogisticRegression()\nmodel.fit(X_train_split, y_train_split)\n\n# Prever no conjunto de valida√ß√£o\ny_val_pred = model.predict(X_val_split)\n\n# Avaliar o modelo usando F1-score\nf1 = f1_score(y_val_split, y_val_pred)\nprint(f'F1 Score: {f1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T19:52:51.154418Z","iopub.execute_input":"2025-03-11T19:52:51.154850Z","iopub.status.idle":"2025-03-11T19:52:51.233901Z","shell.execute_reply.started":"2025-03-11T19:52:51.154814Z","shell.execute_reply":"2025-03-11T19:52:51.232729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Passo 6: Fazer Previs√µes no Conjunto de Teste","metadata":{}},{"cell_type":"code","source":"# Prever no conjunto de teste\ntest_predictions = model.predict(X_test)\n\n# Criar o arquivo de submiss√£o\nsubmission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T19:53:18.274975Z","iopub.execute_input":"2025-03-11T19:53:18.275410Z","iopub.status.idle":"2025-03-11T19:53:18.288764Z","shell.execute_reply.started":"2025-03-11T19:53:18.275364Z","shell.execute_reply":"2025-03-11T19:53:18.287557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîπ Melhorias Implementadas:\nDistribui√ß√£o das classes (sem seaborn)\n\nDistribui√ß√£o do comprimento dos textos\n\nNuvem de palavras para cada classe\n\nGr√°fico interativo de palavras mais frequentes","metadata":{}},{"cell_type":"code","source":"# Importar Bibliotecas Adicionais\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly.express as px\nfrom collections import Counter\n\n# 1Ô∏è‚É£ Distribui√ß√£o das Classes (Usando Matplotlib)\nclass_counts = np.bincount(train_data['target'])\n\nplt.figure(figsize=(6, 4))\nplt.barh(['N√£o relacionado a desastre', 'Relacionado a desastre'], class_counts, color=['blue', 'red'])\nplt.xlabel('Quantidade')\nplt.title('Distribui√ß√£o das Classes')\nplt.show()\n\n# 2Ô∏è‚É£ Distribui√ß√£o do Comprimento dos Textos\ntrain_data['text_length'] = train_data['text'].apply(len)\n\nplt.figure(figsize=(8, 5))\nplt.hist(train_data['text_length'], bins=30, color='purple', alpha=0.7)\nplt.xlabel('Comprimento do Texto')\nplt.ylabel('Frequ√™ncia')\nplt.title('Distribui√ß√£o do Tamanho das Mensagens')\nplt.show()\n\n# 3Ô∏è‚É£ Nuvem de Palavras por Classe\ndef gerar_nuvem_texto(texto, titulo):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(texto))\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(titulo)\n    plt.show()\n\n# Criar nuvem de palavras para cada classe\ngerar_nuvem_texto(train_data[train_data['target'] == 0]['text'], 'Palavras Frequentes - N√£o Relacionadas a Desastres')\ngerar_nuvem_texto(train_data[train_data['target'] == 1]['text'], 'Palavras Frequentes - Relacionadas a Desastres')\n\n# 4Ô∏è‚É£ Gr√°fico Interativo com Plotly - Palavras Mais Frequentes\ndef obter_palavras_frequentes(textos, n=20):\n    palavras = ' '.join(textos).split()\n    contagem = Counter(palavras)\n    return contagem.most_common(n)\n\n# Coletar palavras mais frequentes\npalavras_comuns = obter_palavras_frequentes(train_data['text'])\n\n# Converter em DataFrame (sem usar pandas)\nwords, counts = zip(*palavras_comuns)\n\n# Criar gr√°fico interativo\nfig = px.bar(x=words, y=counts, labels={'x': 'Palavras', 'y': 'Frequ√™ncia'}, title='Palavras Mais Frequentes nos Tweets')\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T20:06:26.345531Z","iopub.execute_input":"2025-03-11T20:06:26.346003Z","iopub.status.idle":"2025-03-11T20:06:33.390385Z","shell.execute_reply.started":"2025-03-11T20:06:26.345971Z","shell.execute_reply":"2025-03-11T20:06:33.389206Z"}},"outputs":[],"execution_count":null}]}